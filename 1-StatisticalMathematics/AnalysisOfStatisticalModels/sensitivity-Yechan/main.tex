\documentclass{article}
\input{preambles.tex}
\title{Dynamic confounding and Long term treatment effect estimation by data combination: point and partial identification}
\author{yechanparkjp}
\date{October 2022}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{comment}
\excludecomment{en-text}

%\bibliographystyle{apalike}


\begin{document}
\maketitle

\begin{abstract}

    We combine experimental and obsevational data that have complementary role in estimating long term treatment effects. The underlying important objective is to higlight the decade old idea of the inherently complementary role that the experiments and economic theory should play in program evaluation in a newly emerging setting of long term treatment effect by data combination. Economic theory provides way to differentiate and choose between nonested competing identification strategies, and can posit a reliable model for the dynamics of labor outcomes even under selection. On the other hand, the exogenous variation of the experimental data can provide ways to choosing between different functional and distributional assumptions that economic models place, and also provides great credibility for tightening bounds even under no point identification.
\end{abstract}

\section{Introduction}

We are interested in assesing the long term effect of a job training program. We have a result from a randomized control trial which measured the change in unemployment rate a year after the program participation, but no long term follow up has been, possibly due to cost of follow up. On the other hand, observational data documenting the results up to 3 years is available, but probably contaminated by self-selection of workers into the program. Is there anyway to get the selection corrected long term effect of the job training program by combining the two data? This is a question that has received considerable interest recently, reflected in the increasing number of papers that have documented conditions under which the long term average treatment effect can be nonparametrically point identified. One of the major and leading works by \cite{athey 2020 long}  provides causal assumptions of internal and external validity of experimental data, and a form of conditional internal validity called latent unconfounding assumptions. While illuminating, there are assumptioins like external validity or latent unconfounding assumption that have not been explicitly placed in the traditional program evaluation, and we would like to look deeper into the nature of these assumptions, and what to do when the assumptions seems to be difficult to hold in the application of interest. 

Specifically, we first look into the external validity assumption, and (a) show that their external validity of long term potential outcome was actually unnecessary for average treatment effect estimation (b) under the relaxed assumption, provide a formal test based on partial identification literature, providing \textit{uniformly sharp identification bounds valid even in the high dimensional regime}

Next, we will investigate the latent unconfounding assumption. We illustrate how it may be difficult to hold for time varying unobserved confounders or unobserved confounding that simultaneously affect the treatment, short, and long term outcome. To highlight this point, we turn from identification of average treatment effect to a average treatment effect on the treated survivors(ATETS), a modified form of average treatment effect that highlights the difficulty of dealing with dynamic confounding, and is of interest in its own right when unemployment rate is concerened. 

We will approach identification of this challenging in three ways. The first two aims for point identification. The first approach aims for assumtions in the form of potential outcomes that is standard in the current literature of program evaluation. We show that the previously maintained assumptions by \cite{athey2020combining}is insufficient for point identification, due to the counterfactual quantity in the conditioning set, and provide additional assumption that suffice for point identificatio, though may be hard to hold in many settings. The second approach turns to more traditional economic modeling, by constructing a dyanmic analogue of the generalized roy model\cite{heckman2007dynamic} \cite{roy1951some}that can flexibly embed assumptions motivated by economic theory on the time varying confounding, The final approach gives up point identification, and provides \textit{sharp identification bounds }on the treatment effect quantity with economically motivated shape restrictions in increasing strength, elucidating the role each identifying assumption plays in shaping inference.

The rest of the paper is organized as follows : Section 2 provides a short introduction to the identification approach of our baseline paper \cite{athey2020combining}, Section 3 discusses the external validity assumption , Section 4 discusses the nature of latent unconfounding assumptions. Section 5 motivates the modified average treatment effect estimand and the experimental approach of identification. Section 6 illustrates the economic modeling approach. Section 7 discusses partial identification strategy. Section 8 discusses the estiimation strategy, and Section 9 shows the empirical application. Section 10 concludes.

\section{Setup}

First we introduce the general notation that will be used throughout our paper.
Given a probability space $(\Om,\F,\P)$, \todo

A researcher conducts a randomized experiment aimed at assessing the effeccts of a policy or intervention. For each individual in the experiment, they measure a q-vector $X_i$ of pre-treatment covaraites, a binary variable $W_i$ denoting assignment to treatment, and a d-vector $Y_{1i}$ of of short term post-treatment outcomes. The researcher is interested in the effect of the treatment on a scalar long term post treatment outcome $Y_{2i} $ that is not meaured in the experimental data. They are able to obtain an auxiliary, observational data set containing measurements, for a separate population of individuals, which consist of the same covaraites $X_i $,treatment, $W_i$, short term outcome $Y_{1i} $ and also the long term outcome $Y_{2i}$. 
For the introduction, the data available for our case is presented in Figure 2. 


As aforementioned, we have two data sets. 

We first introduce the identification assumptioin of \cite{athey2020combining} to identify the long-term ATE. They employ the following four assumptions.
\cite{athey2020combining} 
\begin{assumption}[Experimental internal validity]\label{ass: exp1}
    $W \amalg Y_2(1), Y_2(0), Y_1(1), Y_1(0) | X, G=0$.
\end{assumption}

This will be satisfied by construction in cases where 

\begin{assumption}[Exernal validity of experiment]\label{ass: ex1}
    $G \amalg Y_2(1), Y_2(0), Y_1(0), Y_1(0) | X$.
\end{assumption}

\begin{assumption}[strict overlap]
    The probability of being assigned to treatment or of being measured in the observational data set is strictly bounded away from zero and one, i.e., for each w and g in \{0,1\}, the conditional probabilities 
    \begin{equation}
        P(W = w | Y_1(1), X, G = g ) \text{and} P(G=g | Y_1(1), X, W = w) 
    \end{equation}
    are bounded between $\epsilon $ and $ 1 - \epsilon $ , $ \lambda$- almost surely, for some fixed constant $ 0 < \epsilon < 1/2 $.
\end{assumption}

\begin{assumption}[latent unconfounding] \label{ass:latent}
    $W \amalg Y_2(w) \mid Y_1(w), X, G=1\quad(w =0,1)$.
\end{assumption}

In Theorem 1 of \cite{athey2020combining}, they prove the following identification results. 
\begin{theorem}( \cite{athey2020combining} Theorem 1)\\
$E[ Y_2(1) | G = 1 ]$ is nonparametrically identified. 
\end{theorem}
(Proof)
This itself is very insightful, one of the early works that proposed an effective usage of experimental and observational data. However, at the same time, we notice that assumptions not commonly made in the conventional settings are necessary for identification. In particular, a cross locational independence assumption (Assumption \ref{ass: ex1}), and a form of unconfoudning conditioned on a \textit{latent variable} (Assumption \ref{ass:latent}) appears to be quite new, and may or may not hold depending on certain contexts. We examine both of them in the subsequent sections, how we may be able to relax, interpret, or test these assumptions.

\section{External validity}
In this section, we will focus on the external validity assumption(Assumption \ref{ass: ex1}). We specifically do two things. We first show that the conditional independence assumption between $G$ and $(Y_2(1), Y_1(1)$ can be relaxed to only that of $G$ and $Y_1(1)$. Second, based on those relaxations, we consider various partial identifying assumptions specialized for this setting accompanied with  sharp identification bounds for them. 

We will first show that Assumption \ref{ass: ex1} can be relaxed  of only the short term outcome, formalized in the following assumption and proposition. The key intuition for why this holds is to notice that the external validity assumptions is used in the identification proof of \cite{athey2020combining} two times, and the aggregate bias that appears without the external validity with the long term potential outcome can be shown to be of a simple form, which exactly becomes zero under the conditional independence of the short-term outcome.
\begin{assumption}[short term external validity]\label{ass: ex2}
    $G \amialg Y_1(1) ,Y_1(0) \mid X$.
\end{assumption}
\begin{proposition}
\textit{ii}    The long term average treatment effect in the observational population $ E[Y_2(1) -Y_2(0) | G=1]$ is nonparametrically identified replacing Assumption \ref{ass: ex1} with Assumption \ref{ass: ex2}.
\end{proposition}

(Proof)
\begin{align*}
    \text{It suffices to show that } E[ Y_2(1) | X, G= 1] \text{ is nonparametrically identified under the relaxed external validity assumptions. We will show that }
    \text{Note that }\\
    &E[ Y_2(1) | G=1 ]\\
=& E [E[ Y_2(1) | X, G=1 ] | G=1] \\
=& E[ E[ Y_2(1) | X, G= 0 ] | G=1] ] + A( A: = E[ E[ Y_2(1) | X, G =1 ] - E[ Y_2(1) | X, G =0 ] | G=1]) \\
=& E[ E[ E[ Y_2(1) |  Y_1(1), X, G= 0 ] | X, G =0] | G=1 ]   + A \\
=& E[ E[ E[ Y_2(1) | Y_1(1), X, G=1 ] | X, G =0] | G=1] + B + A ( B: =E[ E[ E[ Y_2(1)| Y_1(1), X, G=0 ] | X, G = 0] | G = 1] - E[ E[ E[ Y_2(1)| Y_1(1), X, G=1 ] | X, G = 0] | G = 1] ) \\
=& E[ E[ E[ Y_2(1) | Y_1(1), W=1, X, G=1] | X, G=0] | G=1] + B + A ( \because $ Assumption \ref{ass:latent}$ ) \\
=& E[ E[ E[ Y_2(1) | Y_1(1), W=1, X, G=1] | W=1,X, G=0] | G=1] + B + A( \because $Assumption \ref{ass: exp1}$) \\
=& E[ E[ E[ Y_2 | Y_1 , W=1, X, G=1] | W=1, X, G=0] | G=1] + B + A (\because consistency )\\
\text{Because } E[ E[ E[ Y_2 | Y_1 , W=1, X, G=1] | W=1, X, G=0] | G=1] \text{is identified from the data, if we show that B + A =0 under the relaxed external validity assumption, our goal is achieved.}\\
\text{Note that } \\
A + B\\
=& E[ E[ Y_2(1) | X, G=1] - E[ Y_2(1) | X, G= 0 ] | G=1]  \\
& -  E[ E[ Y_2(1)| X, G = 0] | G = 1] - E[ E[ E[ Y_2(1)| Y_1(1), X, G=1 ] | X, G = 0] | G = 1] \\
=& E[ E[ Y_2(1) | X, G=1] | G=1] - E[ E[ E[ Y_2(1)| Y_1(1), X, G=1 ] | X, G = 0] | G = 1] \\
=& E[ Y_2(1) | G=1] - E[ \int \int y_2(1)  p(y_2(1) | y_1(1), X, G=1) p( y_1(1) | X, G=0) dy_2(1) dy_1(1) | G=1] \\
=& E[ Y_2(1) | G=1] - E[ \int \int y_2(1) \frac{ p( y_1(1) | X, G=0)}{ p( y_1(1) | X, G=1) }  p(y_2(1) | y_1(1), X, G=1) p( y_1(1) | X, G=1) dy_2(1) dy_1(1) | G=1]\\
=&E[ Y_2(1) | G=1] - E[ y_2(1) \frac{ p( y_1(1) | X, G=0)}{ p( y_1(1) | X, G=1) } | G=1] \\
=& E[ Y_2(1)( 1 - \frac{ p( y_1(1) | X, G=0)}{ p( y_1(1) | X, G=1) } )| G=1]\\
=& 0 ( \because $Assumption \ref{ass: ex2}$ )
\qed
\end{align*}

Under this relaxation, we show how to test using partial identification similar in spirit to \cite{blundell2007changes}, which used it to test the differential wage gap in the UK among gender.
\subsection{  bounds for testing external validity}
\label{sec:uniform bound 1}
An implication of (the relaxed) external validity is that $ F(Y_1(1) | X, G=1) = F(Y_1(1) | X, G=0) $. Rewriting from what we know, the right hand side is nonparametrically identified because  
\begin{align}
    F(Y_1(1) | X, G=0) &= F(Y_1(1) | W=1, X, G=0 ) (\because Ass \ref{ass: exp1} )\\
    &= F(Y_1| W=1, X, G=0)
\end{align}
For the left hand side, by the law of iterated expectation and chain rule, and consistency
\begin{align}
    &F(Y_1 | W=1,X,G=1) P(W=1 | X, G=1) \\
    &\hspace{2em}+ F(Y_1(1) | W=0 , X, G=1)  P(W=0 | X, G=1)
\end{align}%%% \hspace is abbreviation for "horizontal spacing" (10/27 Hirofumi)
and hence the counterfactual quantity is only $F(Y_1(1) | W=0, X, G=1) $.
A pointwise sharp bound under no additional assumption can be easily seen to be 
\begin{align}
    &F(Y_1 | W=1,X,G=1) P(W=1| X, G=1) \\
    &\hspace{2em}\leq F(Y_1| W=1, X, G=0) \\ 
    &\hspace{2em}\leq F(Y_1 | W=1,X,G=1) P(W=1 | X, G=1) + P(W=0 | X, G=1)
\end{align}
, and was indeed implemented in works like \cite{blundell2007changes}\cite{manski2009identification}

The bound above under no additional assumptions is pointwise sharp for each t. However in practice, this will of ten be quite wide. On the other hand, researchers may have prior beliefs motivated by the economics of the problem, and they are interested in how the assumptions may tranlate into narrower bounds and the results of the test under such assumptioins. The partial identification framework we provide below allows for such procedures in a systematic manner.
 
 We briefly mention the realted literature. There is an increasing amount of research in the sensitivty analysis literature that assess the generalizability, or transportability of a result in one location or one research design, e.g., \cite{imai egami, } etc. However, while the partial identification on the distribution function under various assumption hav been widely used \cite{blundell et al, manski 200 3, manski horo}, there are few papers that consider such framewok for assessing external validity. \cite{manski 2013} provides a framework ofr  'external assesment' using boudns, it takes a more social decision theory, minimax perspective, which is a different approach from ours. 
\begin{en-text}
\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{n=30chishort.png}
    \caption{The monte carlo distribution of the test statistic based on OLS is far from being normal due to the asymmetric and heavy-tailed noise.}
    \label{fig:demonstration}
\end{figure}

\end{en-text}
 The novelty of the assumptions that we consider can be rephrased as the following. consider the diagram below. WHile past literature have focused on the assumption that assess the assumption pertaining to selection (across treatment groups), the random variable W, \cite{blundell et al}, here we also consider the cross location/design assumptions , and moreover on the intersection of those two, delivering sharp identification bounds for each case. As we will see, this is particularly useful, because to gain informative bounds only using cross treatment (W) variation, strong assumptions such as stochastic dominance may be necessary. However, with the aid of additional reasonable cross locational varation assumptions, by taking the intersection, we could get informative bounds even under no clearly implausible assumptions. 
- We will propose the assumtions here and provide the sharp bounds in the subsequent sections
    - \begin{enumerate}[a]
     \item cross treatment (W) assumptions
     \begin{enumerate}
     \item stochastic dominance 
     $F(Y_1(1) \leq t |X=x W=1, G=1 )\leq F(Y_1(1) \leq t |X=x ,W= 0, G=1) \forall t, x $
        - this is a form of negative selection into treatment, which in the case of job training program, past research like \cite{ashenfelter 1985, card, heckman 1999} have showed how people with lower potential wage or employment probability tend to apply for job training, for all covariates, and quantiles of the potential outcome
        - if one thinks that this result holding uniformly might be too stringent, but is convinced that at least for the lower 25 percent quantile of the treated, such negative selection could be true, then we could consider the following
        - \item  lower quantile dominance
        - $F(Y_1(1) \leq t |X=x W=1, G=1 )\leq F(Y_1(1) \leq t |X=x ,W= 0, G=1) \forall t \leq t_{0.25} , x,  $where $ t_{0.25} $is the lower 25 percent quantile of$ F(Y_1(1) \leq t |X=x W=1, G=1 )$, which is observable. 
        -$ Moreover, one could consider the monotone iv assumption, which states that for a particular obserbale covariate the value of which is indicative of the unobserved potential outcomes distribution, formalized below$
         \item monotone iv 
        - $ $v_1 \leq v_2 \implies  F(Y_1(1) \leq t |V =v_1 X=x ,W= 0, G=1)  \leq F(Y_1(1) \leq t |V =v_2 X=x ,W= 0, G=1)  \forall v_1, v_2 \in \mathcal{V}$ 
        - in our problem the V may be a baseline covariate like test scores in the past, with lower values in such test scores may indicate lower values of the unobserved employment rate
         \end{enumerate} 
        - \item cross location/design(G) assumptions
         \begin{enumerate}
         \item hetereogeneity
         $ \exists t_0 [ \forall t \leq t_0 :F(Y_1(1) \leq t |V = X=x ,W= 0, G=1) \leq F(Y_1(1) \leq t |V = X=x ,W= 0, G=0)   \land \forall t \geq t_0 : F(Y_1(1) \leq t |V = X=x ,W= 0, G=0) \leq F(Y_1(1) \leq t |V = X=x ,W= 0, G=1) \forall x \in \mathcal{X}]$
        - this to our knowledge is new, and is captured in the illustration below. It is often the case that the observational data have people with covariates with wider range, dispered and heterogenous, while the experimental data based on eligitibility rules and the desing of experiments are restricted to a more concentrated covariate value. In that case, the distribution of the observational quantile would be much wider than the experimental and before and after some threshold $t_0$, the cumulative probability may be reverse. 
         \end{enumerate}
         \end{enumerate}
        - 
    - 
























\subsection{point wise sharp bounds}
\subsection{uniformly sharp bounds}
in the previous section, we have provided point wise sharp bounds
However, if we see the bound as a bound on a function(in this case the distribution function), the pointwise bound contains is not sharp in that it can contain distribution functions that is not compatible with the data. 
For example, it does not impose the restriction that for any $ t_0 \le t_1$, 
\begin{align}
\label{eq: partial id necc dist sharp}
    P( t_0 \leq Y_1(1) \leq t_1 | \mathbf{x} ) \geq P ( t_0 \leq Y_1(1) \leq t_1 | \mathbf{x} = x, W = 1) P ( W =1 | \mathbf{x} = x)
\end{align}
This is easiest to see in a simple counterexample as in the following: 
omit $\mathbf{x}$, and G=1 for simplicity, and let $P ( \mathbf{W} = 1 ) = \frac{2}{3} $ and let 
\begin{align}
    P (Y_1(1) \leq t | W=1)=
    \begin{cases}
    0&\ifquad t<0\\
    \frac{1}{3}t&\ifquad 0< t< 3\\
    1 & \ifquad t \geq 3
    \end{cases}
\end{align}


Consider the distirbution function 
\begin{align}
    F(t) = \begin{cases}
        0&\ifquad t < 0\\
        \frac{5}{9}t & \ifquad 0 \leq t < 1 \\
        \frac{1}{9}t + \frac{4}{9} & \ifquad  1 \leq t \leq 2\\
        \frac{1}{3}t & \ifquad  2\leq t < 3\\
        1 & \ifquad t \geq 3
    \end{cases}
\end{align}
For each $t \in\R$ , F(t)  lies in the tube defined by the pointwise sharp bound above. However it cannot be the CDF of $ Y_1$, because F(2) - F(1) = $ \frac{1}{9}< P (1 \leq Y_1(1) \leq 2 | W=1) P( W=1)  $ , directly contradicting (\ref{eq: partial id necc dist sharp})
What is then the uniformly sharp identification bound in this case? THe following proposition provides an answer.
\begin{proposition}
    The uniformly sharp identification bound for $ F(Y_1(1) |  X, G=1)$under the maintained assumptions in \cite{athey2020combining} 
    \begin{equation}
        \mathcal{H}_P[ Q(y |   =  \tau(x) \in \mathcal{T} : \tau_{K(x) }\geq \mathrm{P}( Y_1(1) \in K | \mathbf{x}=x, W=1,G=1) \mathrm{P}(W =1 | \mathbf{x} = x, G=1) , \forall K \subset \mathcal{Y} , \text{ where }\mathcal{Y}= \{ \{0,1\} \}
    \end{equation} 
\end{proposition}
\begin{proof}
    
\end{proof}


This shows how intuition do not necessarily yield the desirable uniformly sharp bounds. I therefore rely mainly on a newly emerging approach from random set theory\cite{molchanov2005theory} to mechanimcally derive sharp bounds. 


I provide the fundamental definitions and theorems that I will use below, and refer the reader to \cite{molchanov2005theory,molchanov2018random} for the proofs and more in depth explanations
\begin{definition}[Random closed set]
    A map $\mathbf{X} $from a probabiity space $( \Om,\F,\P ) $ to the family $\mathbf{F} $ of closed subsets of $ \R^d $ is called a random closed set if 
    \begin{equation}
        \mathbf{ X}^-(K) = \{ \om \in \Om : \mathbf{X} (\om) \cup K \neq \emptyset \}     
    \end{equation}
    belongs to the $\sigma $-algebra $\mathfrak{F} $ on $\Om$ for each compact set $K$ in $\mathbb{R}^d$
\end{definition}

\begin{definition}[Capacity functional and containment functional]\mbox{}
    \begin{enumerate}[1.]
        \item A functional $T_\mathbf{X}( K) : \mathcal{K}  \to [0,1] $ given by 
        \begin{equation}
            \mathrm{T}_{\mathbf{X}}( K) = \mathbb{P} \{ \mathbf{X} \cup K \neq \emptyset \}, K \in \mathcal{K} 
        \end{equation}
        is called \textbf{ capacity(or hitting) functional} of $\mathbf{X}$
        \item A functional $\mathrm{C}_\mathbf{X}(F) : \mathcal{F} \to [0,1] $given by \begin{equation}
            \mathrm{C}_\mathbf{X}(F) = \mathbb{P}\{ \mathbf{X} \in F\}, F \in \mathcal{F}
        \end{equation} 
        is called the \textbf{containment functional } of $\mathbf{X}$.
    \end{enumerate}
\end{definition}

\begin{definition}[Measurable selection] 
For any random set $\mathbf{X}$, a (measurable) selection of $\mathbf{X}$ is a random elemnt $\mathbf{x}$ with values in $\mathbb{R}^d $such that $\mathbf{x}(\omega) \in \mathbf{X}( \omega)$ almost surely. I denote by $\Sel( \mathbf{X} ) $the set of all selections from $\mathbf{X}.$
\end{definition}

below we have the fundamental theorem we heavily use to characterize sharp identification bounds. 
\begin{theorem}[Artstein inequality]\label{eq:artgen}
    A probability distribution $\mu$ on $ \mathbb{R}^d$ is the distribution of a selectio nof a random closed set $\mathbf{X}$ in $\mathbb{R}^d$ if and only if
    \begin{equation}
        \mu(K) \leq \mathrm{T}(K) = \mathbb{P} \{ \mathbf{X} \cup K \neq \emptyset \}
    \end{equation}
    for all compact sets $K \subseteq \mathbb{R}^d $. Equivalently, if and only if
    \begin{equation}
    \label{eq: art con1}
        \mu(F) \geq \mathrm{C}(F) = \mathbb{P} \{ \mathbf{X} \subset F \}
    \end{equation}
    for all closed sets $ F \subset \mathbb{R}^d $ If $ \mathbf{X} $ is a compact random closed set, it suffices to check \ref{eq: art con1} for compact sets F only
\end{theorem}

\section{Latent unconfounding assumption}











     Different from the previous section, we will focus on the internal validity of the observational data, i.e., under what conditions does W be independent of $Y_t(w)$ ?
     As hinted in the introduction, a problem of nonnested approaches under the same availability of data may be the first issue that practitioners have to deal with when having the dataset and deciding ways for establishing identification.
    Specifically, the alternative approach for identification of the long term ATE was recently proposed by \cite{ghassami et al 2022} , which posited an \textit{equi-confounding bias assumption}, which is a form of parallel trends assumption applied to the data-combination setting,formally presented below.
    \begin{assumption}(Equiconfounding bias assumption)
     \label{ass: equicon}
    $ (i) E[Y_2(0) - Y_1(0)| G= 1] = E[Y_2(0) - Y_1(0) | G=0] \\
     (ii) E[Y_2(1) - Y_1(1)| G= 1] = E[Y_2(1) - Y_1(1) | G=0]$
     \end{assumption}
They proved the following theorem in their paper.
 \begin{theorem}
Replacing latent unconfounding assumption with \ref{ass: equicon}(i)and maintaining all the other assumptions, the long term ATT is identified.\\
Replacing latent unconfounding assumption with \ref{ass: equicon}(i) (ii), and maintaining all the other assumptions, the long term ATE is identified.
 \end{theorem}
The equiconfounding assumption can be intuitively explained that the potential growth between the short term and long term outcome is the same among the treated and the untreated. Usually only the the untreated potential outcome (i) is maintained to identify the ATT in the standard did setup. Additionally assuming the equivalent growth among the treated potential outcomes identify the ATE , the proof of which is in their paper \cite{ghassami 202}
While this is interesting, it leads to an additional problem coming from the nonnested nature of equi-confounding and the latent unconfounding assumption. The latent unconfounding has us to imagine what happens under condition on an \textit{latent potential outcome}, while the equiconfounding assumption has us to think about the dynamics between two post-treatments, arguably more complicated than the canonical parallel trends in the usual did setup.  How should a policy maker be able to map these various assumptions onto their current problem? One solution that I posit in the following is that, taking into account the selection mechanism provides insight into when exactly these assumptions hold or fail to hold.

I illustrate this idea in two caonical selection mechanims that are commonly used in empirical research, namely that of \cite{ashenfelter1985susing}, and the Roy model \cite{roy1951some}.
I provide \koko(necessary and sufficient?) conditions under which each assumption holds.

\koko(covariates)

Formally, for the main section, we model the potential outcome as $Y_{it} ( 0) = \alpha_i + \lambda_t + \alpha_i \lambda_t +\epsilon_{it}  , E[ \epsilon_{it} ] = 0 , i \in [0, N] , t =1,2$, $Y_{it}(1) = Y_{it}(0) + \delta_{it} $, which is generalizes the classical two-way-fixed effect model by (i) allowing for interactive fixed effect as in \cite{bai2009panel,abadie2021using}(ii) allowing for arbitrary treatment effect heterogeneity while the two-way fixed effect model usually assumes constant treatment effect. 
We first consider the selection mechanism proposed in \cite{ashenfelter1985susing} 
In \cite{ashenfelter1985susing} page \koko , the selection mechanism was posited as 
\begin{equation}
    W_i = 1\{ Y_{i1}(0) + \beta Y_{i2} ( 0) \leq c \} =  1 \{  (1 + \beta ) \alpha_i + \epsilon_{i1} + \beta \epsilon_{i2} \leq \tilde{c} \} ,\\
    \text{where} \beta \in [0,1] \text{is a discount factor and } \tilde{c} = c - \lambda_1 - \beta \lambda_2 .\\
    
\end{equation}

The following theorem can be shown.
\begin{theorem}
Consider the setting specified above. Then Latent unconfounding holds iff $\beta =0$. Equi-confounding assumption does not hold for any 
\end{theorem}
(Proof)
Note that\\
$W_i = 1\{ Y_{i1} + \beta Y_{i2} > 0\} = 1\{ \alpha_i ( 1 + \lambda_1 + \beta( 1 + \lambda_2 ) ) + \lambda_1 + \beta \lambda_2 + \epsilon_{i1} + \beta \epsilon_{i2} > 0 \} $
When $\beta=0$, we have that $W_i =$
Next, we consider the Roy selection mechanism\cite{roy1951some}, with its slight extension by e.g., \cite{heckman1984method} . The essence of the Roy selection model is that, it is based on \textit{treatment effects}, opposed to the untreated potential outcome in \cite{card ashenfelter}



I fix the outcome model to be a canonical non-separable panel data model $Y_{it} (0) = \alpha_i + \lambda_t + \alpha_i \lambda_t + \epsilon_{it} $, where $\alpha_i $ is the individual fixed effect, $\lambda_t$is the time fixed effect, which we regard as non-stochastic (by conditioning on its realizations) as commonly assumed, in e.g., \cite{citeghanem2022, bonhomme}. The 

To do this, I embed the two different approaches for ATE estimation in a first separable, then nonseperable panel data model ,  explcitly modeling the constant and time varying observables and unobservables and when the latent unconfounding and the equiconfounding hold or not hold in the respective settings.  





\subsection{comparison of latent unconfounding and equiconfounding in nonseprabale panel data setting}
 following the convention of did in assuming only the parallel trend on the untreated (i) ,and focusing on ATT, although the ATE will also follow with a similar setup
 we formalize the problem as the following.
     we first assume a model for potential outcomes that is separable in the timeinvariant and timevarying unobsrevables
    \begin{assumption}
     $Y_{it} ( 0) = \alpha_i + \lambda_t + \alpha_i \lambda_t +\epsilon_{it}  , E[ \epsilon_{it} ] = 0 , i \in [0, N] , t =1,2$
     \end{assumption}
 in the above asumption, $\alpha_i$ is the time invaraint unobserable, $\lambda_t$ is the (nonstochastic, without l.o.g) tiime fixed effect, and $\epsilon_{it} $ is the tie varying individual specific unobservable. This is close to the commonly assumed two way fixed effect model, only that treatment effect heterogeneity is allowed
 
 next we model the selection mechanism W, as 

$ W_i = w( \alpha_i, \epsilon_{i1}, \epsilon_{i2}, \nu_i, \eta_{i1}, \eta_{i2} )  $, \\
where the selection into treatment may in general   depend on the unobservable determinants of the untreated potential outcomes  \\
 $(\alpha_i, \epsilon_{i1}, \epsilon_{i2} )$\\
as well as additional timeinvariant and timevarying vectors of random variables $ (\nu_i, \eta_{i1}, \eta_{i2} )$

 This general selection mechanism accommodats many different types of selection, including , random assignement ,selection based on past outcomes, roystyle selections basd on treatment effects and othe selection mechanisms based on economc decision problems( \cite{heckman and robb} )
 
 In the case of the equiconfouding assumption(i) , a necessary sufficient condition to hold is provided in \cite{ghanem et al} as in the following
 \begin{theorem}
 Suppose that the separable outcome and the general selection mechansim holds. Suppose further that $P(G_i = 1) \in (0,1) , \nu_i^1 amalg ( \alpha_i , \epsilon_{i1}, \epsilon_{i2} ) P( \nu_i^1 > c) \in (0,1) $ for some c $ in \mathbb{R}, and P (\epsilon_{i2} \geq \epsilon_{i1} ) > 0.$ Then equiconfounding assumption (i) holds if and only if $\epsilon_{i1} = \epsilon_{i2} a.s.$
 \end{theorem}
 
 

\subsubsection{example: selection mechanism of ashenfelter and card(1985)}
In \cite{ashenfelter and card 19 85} , the selection mechanism was posited as 
\begin{equation}
    W_i = 1\{ Y_{i1}(0) + \beta Y_{i2} ( 0) \leq c \} =  1 \{  (1 + \beta ) \alpha_i + \epsilon_{i1} + \beta \epsilon_{i2} \leq \tilde{c} \} ,\\
    \text{where} \beta \in [0,1] \text{is a discount factor and } \tilde{c} = c - \lambda_1 - \beta \lambda_2 .\\
    
\end{equation}
In this case, selection depens only on the unobservable determinants of the untreated potential outcomes $( \alpha_i , \epsilon_{i1}, \epsilon_{i2} ) $ .

When $\beta = 0 $, so that selection depends only on $Y_{i1} (0) $, 
\begin{equation}
    W_i = 1 \{ Y_{i1}0) \leq c \} = 1 \{ \alpha_i  + \epsilon_{i1} \leq \tilde{c} \}
    
\end{equation}
In this case, the latent unconfounding assumptions is satisfied without any further assumptions. 

On the other hand,  the equiconfounding assumption requires the following additional assumptions.
\begin{enumerate}
    \item $ ( \nu_i, \eta_{i1} , \eta_{i2} ) | \alpha_i , \epsilon_{i1}, , \epsilon_{i2} =^d ( \nu_i , \eta_{i1}, \eta_{i2} ) | \alpha_i$
    \item $ E [ \epsilon_{i2} | \alpha_i , \epsilon_{i1} ] = \epsilon_{i1}$
\end{enumerate}








\section{ATE on the treated survivors :motivation}

\begin{definition}
    (Average treatment effect on the treated survivors(ATETS))\\
    $  E[Y_2(1) =1| Y_1(1)=1, G=1 ] - E[ Y_2(0) = 1 | Y_1(1) =1, G=1] $
\end{definition}

We note that if this causal estimand may be of quite interest. It provides us with what will be the causal effect on the transition probability from unemployment to employment for the \todo{ people who would have remain unemployed in the short term}. Policymakers must take into consideration such quantities, since the policy that seemed to have no effect in the short term, may turn out to be very effective later on, and vice versa. \\
However, it can be easily seen that there is a fundamental challenge for identification.
The crucial point here is the $ E [ Y_2(0) = 1 | Y_1(1) = 1] $ part, where while the $Y_1(1) $ 
is the potential outcome for the treated case,whereas $ Y_2(0) $ is the untreated . In other words, there are two counterfactual worlds coinciding in this case, which is at the heart of the fundamental problem of casual inference \cite{imbens2015causal} \\

It should be quite clear that only under the maintained assumptions in the previous section, this quantity is not nonparametrically identified under the observaional population G=1 because in any case the  $Y_1(1)$ cannot be reduced to a factual quantity when $ W \amalg Y_2(1) ,Y_2(0), Y_1(1), Y_1(0) |G=1$. does not hold.
Is there any way to nonparametrically identify this quantity under additional assumptions?\\
One possibility may be the no state dependence assumption that has be introduced and elaborated in papers like \cite{heckman1981heterogeneity,heckman1984method,torgovitsky2019nonparametric}. The issue in the literature is whether the commonly observed serial correlation between sequential outcomes come from unobserved heterogeneity , that affects both treatment and all the subsequent outcomes, or that there is state dependence, that the fact that one is placed in a state of unemployment in the previous stage itself has an effect on the possibility of unemployment this term(e.g. , in the form of less opportunity for gaining social skills in unemployment). In the latter case, the past potential outcome can be seen as a randomized treatment for the current potential outcome. \\
If we posit , therefore, that there is no state dependence in our context,i.e. there is no direct (treatment) effect of the past outcome on the current outcome, we could argue that 
\begin{equation}
    (Y_2(1), Y_2(0) ) \amalg (Y_1(1) , Y_1(0) ) | G=1 
\end{equation}
holds, \footnote{  there are few papers that use potential outcome notations to formalize no state dependence, and arguably, the appropriate no state dependence should not be the joint independence across counterfactuals but for each treatment potential outcome, i.e. $ Y_2(w) \amalg Y_2(w) \, for w= 0,1 $. This is similar to the strong ignorability \cite{rosenbaum1983central} and the weak ignorability \cite{imbens2015causal} difference. For our purposes, we will stick with the strong no state dependence assumption in this paper }   

In this case it is easy to see that under the previously maintained assumptions and no state dependence, the ATETS reduces to the usual LTATE(long term average treatment effect) because $E[ Y_2(1) | Y_1(1), G=1 ] = E[ Y_2(1) | G=1] $, and $ E[Y_2(0) | Y_1(0) , G=1] =  E[Y_2(0) |  G=1]$   , and the identification strategy of \cite{athey2020combining} can be used. 



Nevertheless, many empirical papers\cite{heckman1981heterogeneity,heckman1984method, torgovitsky2019nonparametric} have argued from economic theory and empirical evidence how , while the observed serial correlation between outcomes are often cases not solely by unobserved heterogeneity, it has been argued quite strongly that in the unemployemnt context, state dependence is also a crucial factor. 
\\ Therefore, in the next section , we will build economic models that incorporate the rich accumulation of emprical research on unemployment  to gain the joint potential outcomes across counterfactual states, to identify the proposed quantity above.



\koko

\begin{remark}
\koko 
Since transition probability can be interpreted as one form of hazard parameter, naturally is of major concern. First in terms of right censoring, 
The way we calculate the transition probability , we do not have to worry about the censoring, 
\end{remark}
\begin{theorem}
dd
\end{theorem}


\section{economic modeling}
We will consider how short-term experimental and observational data can be combined to get a more reliable causal effect under initial selection and dynamic treatment effects.
 In particular, in this section, we will model a duration model and the ATETS as the coefficient on the treatment indicator in a duration model
 
First we see how economic theory can play a role in the initial stage when the interest is in the job training program effect on unemployment.
      For unemployment, ample economic theory and evidence suggests negative duration dependence due to the so-called 'weed-out effect'. The intuition is that, as time passes, the more competent people are likely to leave first, and the people with lower latent ability remain, and they tend to keep being unemployed.
      
         However, there are limitations to economic theory. While it provides the general declining tendency, it has been challenging to choose between different declining hazard models( e.g., PH, MPH, GAFT models) or to choose the precise distribution for the error term or the functional form, especially under selection. This variety of model choices leads to several model specifications with varying estimates, making it difficult to choose the one to report as the point estimate, which motivates the usage of experimental data.
         
             Specifically, we propose the use of short-term experimental data as the criteria to choose among those competing models, which was filtered in the first stage based on the economics of unemployment.
             
             Before going into the two-step procedure in detail, we shortly draw the connection with the past literature about using experimental data to evaluate econometric estimators. In the paper of, for example, \cite{lalonde1986evaluating}, it illustrated when the effect of a job training program on earnings was of interest, the various econometric estimators had varied treatment effects, and it was difficult to choose between those estimators without the 'ground truth' by an RCT. While there have been several papers elaborating on the usefulness of matching estimators or model calibration \cite{heckman1997matching,heckman1998characterizing} of nonexperimental estimators, there is a general consensus of experimental data having external validity.
             The novel contribution of this paper relative to that literature is twofold: (a) while the vast majority of documents taking the \cite{lalonde 1986} approach focuses on outcomes of earnings, this paper focuses on unemployment duration, which seems to be relatively few. A notable exception is by \cite{ham1996effect}. Still, it focuses on noncompliance, a significant problem, but what we abstract from in this paper (b) we specifically focus on the \textit{dynamics  }of treatment effects, in which the previous sections have illustrated how short-term experimental data alone may not fully capture the dynamic confounding that is likely to occur. Therefore, the approach here is to take a two-layer approach to (i) first let the theory guide the general dynamics that is plausible in the form of shape restrictions on the hazard and then (ii) let the short-term experimental data choose between those subtle difference in specifications to capture the baseline confounding or the distributional aspect that is arguably more stable across time. We hope this work spurs more integration of the two approaches to more effectively answer policy-relevant questions, as in \cite{todd2020best}
             
\subsection{specific procedure}
We will briefly overview the different duration models that might be relevant to our current paper.  We will focus on the arguably predominant duration model, mixed proportional hazard model (MPH), and vary various functional forms on it and see how the experimental data can aid in selecting the appropriate one.\\
While there may be various reasons for the popularity of Mixed proportional hazard models in economics, according to \cite{heckman1984method, van2001duration, abbring2003nonparametric} there are mainly two reasons.
Firstly,  it is the most parsimonious model that contains the three indispensible elements; baseline hazard, observed covariates, and unobserved heterogeneity in its model. Secondly, an economic interpretation of the estimates is more amenable relative to other accelerated failure time models or general transformation models that do not distinguish the general and the individual hazard rates.




\section{partial id of ATETS}
- While the previous sections have explored point identification by either placing nonparametric assumptions like no state dependence, or formulated economic models with functional forms on the duration parameters to point identify the estimates, those assumptions may be difficult to hold in practice , at leats exactly. Thus we finally turn to partial identification and provide \textt{sharp identification bounds} on the ATETS with inequality and independence restrictions motivated by economic theory. Particular interest is placed on the identification power of latent unconfounding, and equiconfounding assumption as a \textt{partial identification assumption}.
- From the perspective of the partial identification literature, potential outcomes can be seen as an interval data, with the upper and lower bound determined by the support of$ Y_t(1$) , in this case [0,1]. Partial identification on interval outcomes have been extensively studied from \cite{manski 2003, manski and tamer 2002} to name a few. On the other hand, partial identification on interval covariate outcomes have received much less attention, and for the few research as in \cite{berst molinari 2008}, they have been found to be notoriously complex to analyze. In our case , For quantities like $E[ Y _2(0) | Y_1(1) ]$ ,we have \textt{both interval outcome and covariate}, which highlights the challenge, while the  binary treatment and outcome alleviates such challenges. \cite{manski and horowitz 2000} considers a similar setting but with different assumptions like MCAR .\\
We provide our estimates from the worst case bounds and gradually increasing the strenght of our assumptions.  We first start with assuming that we only have information on the observational data, and in the absence of any further assumptions, what can be said about ATETS = $E[ Y_2(1) | Y_1(1) , G =1 ] - E[ Y_2(0) | Y_1(1), G =1]$ ? We prepare the following lemma from \cite{manski and horowitz 2000} .


\begin{theorem}
- the bounds on $ATETS = E[ Y_2(1) | Y_1(1) , G =1 ] - E[ Y_2(0) | Y_1(1), G =1] a$bsent any assumptions are uniformative, i.e. their range is larger than 1
- Proof \\
\begin{align}
&\text{it suffices to show that } E[ Y_2(0) =1| Y_1(1)=0, G=1] \text{is uninformative.}\\ 
&\text{consider the missingness indicator } Z_x, Z_y, \text{that is equal to 1 if and only if } \\
&\text{the covariate }Y_1(0),\text{ and the outcome} \\
&Y_2(1) \text{is observed, and 0 otherwise} \\
&\text{then , by an application of bayes rule,and law of total probability}\\
&E[ Y_2(0)=1 | Y_1(1)=0, G=1] \\
&= \sum_{j,k} \frac{ P (Y_2 =1 | Y_1 = 0 , Z_y = j, Z_ x = k ) P ( Y_1 = 0 | Z_y =j,Z_ x =k ) P( Z_y = k, Z_x = j) }{ \sum_{ k, j} P( Y_1 = 0 | Z_y = j, Z_x = k ) P (Z_y = j, Z_x = k) }\\
&\text{ then by applying \cite{horowitz and manski 1995} Corollary 1.1 , we obtain the following sharp bounds :} \\
& P(Y_2 =1 | Y_1 =0, Z_y =1, Z_ x =1 ) P( Y_1 = 0 | Z_y =1, Z_x = 1 )P( Z_y = 1, Z_x =1) \leq \\
& E[ Y_2(0)=1 | Y_1(1)=0, G=1] \\
&\leq P( Y_1 = 0 | Z_y = 0 ,Z_x = 1) P( Z_y = 0 , Z_x =1 ) + P( Z_y = 0, Z _ x = 0) \\
&+ P( Y_2 =1 | Z_y = 1, Z_ x =0) P  Z_ y =1, Z_x = 0)  \\
&\text{then by \cite{horowitz and manski 2000} Corollary 1.1,}\\
&\text{we can conlude that the bounds are uninformative} \qed 
\end{align} 
\end{theorem}






Next, we will look at how the bounds tighten we have the additional information of auxiliary experimental data. 

The sharp bound under experimental data, which entails the conditional independence of experimental internal validity $ W \amalg Y_2(1), Y_1(1) | ,G=0$ and external validity \footnote{in line with the section after the discussion on external validity, we will maintain this assumption in the main body, but it is certainly of interest how tight the bounds withought the external validity assumption.
}






\section{Estiimation}

\subsection{ uniform inference on the confidence band }-
in this section, we provide inference on the external validity bounds provided in Section\ref{sec:uniform bound 1} that allows for possible high dimensional covariates.  We adopt the general framework in \cite{chernozhukov honest} to build adaptive uniform bands under the assumption of self-similarity , th which uses a constant $\epsilon$ > 0 to rule out functions such that the level of regularity is statistically difficult to detect. \cite{picard tirbou, armstrong 2021} . This is one way to tackle the impossibility result on truly adaptive confidence bands proved in papers like \cite{low 1997} 
- We genrally follow the estimiation procedure in Section 3 of \cite{chernozhukov 2014} for constructing the bounds and getting the critical values that is based on widening the bands based on the etimated bias. We first choose  the number of bases that is selected by the lepski method , and then use sieve estimation with fourier bases  to estimate the upper and lower bounds, and the critical value obtained the gaussian multiplier bootstrap. 


\subsection{estimation of ATETS under no state dependencel}
We provide the nonparametric influence function for estimating the ATETS, which coincides with the ATE . One paper we know that have derived the nonparametric influenc function for this estimand is \cite{chen2021semiparametric} , which uses the projection upon the tangent space approach, and using an adhoc guess and verify approach to getting the influence function. Here we provide an alternative, quicker, and mechanical approach to deriving the influence function using the path-wise derivative calculation, similar in spirit to \qu \cite{ichimura2022influence}
Note that in the nonparametric case, the influence function in the tangent space is unique, so the mean zero function with the appropriate inner product with the score will be what we wanted.
\todo 
\begin{theorem} 
    The efficient influence function for $\tau = E[ Y_2(1) | Y_1(1) , G=1] - E[ Y_2(0) | Y_1(1)   ] $ is 
    \begin{align}
        &\frac{ g w }{ p(G=1) } \Square{ (y_2 - E[ Y_2 | W=1 , Y_1 , X, G =1]  ) \frac{ P( G =1 | Y_1 , W=1 , X ) }{ P(G=1 | Y_1 , W=1 , X ) } P( W=1 | X , G=1)  \right. \\
        &\left.+ E [ E[ Y_2 | W = 1, Y_1 , X, G =1 | W =1, X, G= 0 ] ] - \tau} \\
        &+\frac{ (1- g) P(G = 1|X)   }{ P(G= 1) P ( G =0 |X) } \paren{ \frac{ w ( E [ Y _ 2 | Y_1,W =1 , X, G =1 ] - E [ E[ Y_2 | Y_1, W=1, X, G=1 ] | W =1, X, G= 0 ] ] ) }{ p ( W =1 | X, G =0 ) } }
    \end{align}
\end{theorem}
%%% "\paren{}"と"\Square{}"を使用(10/27 Hirofumi)．
\begin{proof}
    Based on the argument in Section \qu , by symmetry, it suffices to show the influence function  for $E[ Y_2(1) | Y_1(1) , G=1] = E[ E[E [ Y_2 | Y_1 ,W=1,X, G=1] | W=1, X, G=0 ] | G=1]$
    we use $ \partial_t f(t) $ to denote $\frac{ \partial f(t) }{ \partial_t } |_{t= 0 } $ For parameter $\theta $, let $ \theta_t $ be the parameter under a regular parametric sub-model indexed by $t$, that includes the ground-truth model at $t=0$.
    Let $V$ be the set of all observed variables. In order to obtain the influence function, we need to find a random variable $\psi $ with mean zero, that satisfies, 
    \begin{equation}
        \partial_t \psi_t = E [ \psi S(V) ]
    \end{equation}
    where $ S(V) = \partial_t log p_t (V) $.
    To simplify notation, we assume that all variables are discrete. 

    First note that 
    \begin{align}
        \partial_t \psi_t = \partial_t \sum_ { y_2, y_1,x} y_2 p_t ( y_2| y_1 , W= 1, X , G= 1) p_t ( y_1 | W =1 , x, G= 0 ) p_t( x | G =1 )   \\
        = \label{eq: eif first chain} \sum_ { y_2 ,y_1,x} y_2 \partial_t p_t ( y_2| y_1 , W= 1, X , G= 1) p ( y_1 | W =1 , x, G= 0 ) p( x | G =1 )   \\
        + \sum_ { y_2 ,y_1,x} y_2  p ( y_2| y_1 , W= 1, X , G= 1)\partial_t p_t ( y_1 | W =1 , x, G= 0 ) p( x | G =1 ) \\
    + \sum_ { y_2 ,y_1,x} y_2  p ( y_2| y_1 , W= 1, X , G= 1) p ( y_1 | W =1 , x, G= 0 ) \partial_t p_t( x | G =1 )
    \end{align}
    for the first term in  \ref{eq: eif first chain},  we have
\begin{align}
&=\sum_ { y_2 ,y_1,x} y_2 \partial_t p_t ( y_2| y_1 , W= 1, X , G= 1) p ( y_1 | W =1 , x, G= 0 ) p( x | G =1 ) \\
&= \sum_ { y_2 ,y_1,x} y_2 p ( y_2| y_1 , W= 1, X , G= 1) p ( y_1 | W =1 , x, G= 0 ) p( x | G =1 ) S ( y_2| y_1 , W= 1, X , G= 1) \\
&= \sum_ { y_2 ,y_1,w, x, g}  \frac{ w g }{ p (G = 1) } y_2  \frac{ p( y_2, w | y_1, x, g}{ p ( W = 1 | y_1(1) , x , g ) }   p ( y_1 | W =1 , x, G= 0 ) p( x | G =1 ) \frac{ p ( g, x ) } { p(g =1)} S ( y_2| y_1 , W= 1, X , G= 1) \\
&= \sum_ { y_2 ,y_1,w, x, g}  \frac{ w g }{ p (G = 1) } y_2  \frac{ p( y_2, w | y_1, x, g}{ p ( W = 1 | y_1(1) , x , g ) }   p ( y_1 | W =1 , x, G= 0 ) p( x | G =1 ) \frac{ p ( g, x ) } { p(g =1)} S ( y_2| y_1 , W, X , G) \koko \\
&= \sum_ { y_2 ,y_1,w, x, g}  \frac{ w g }{ p (G = 1) } y_2   p( y_2, w | y_1, x, g)  \frac{ P( G =1 | Y_1 , W=1 , X ) }{ P(G=1 | Y_1 , W=1 , X ) } P( W=1 | X , G=1)   \\
&p ( y_1 | W =1 , x, G= 0 ) p( x | G =1 )  
\frac{ p ( g, x ) } { p(G =1)} S ( y_2| y_1 , w, x , g) ( \because bayes rule)\\
&= E [  \frac{ w g }{ p (G = 1) } y_2     \frac{ P( G =1 | Y_1 , W=1 , X ) }{ P(G=1 | Y_1 , W=1 , X ) } P( W=1 | X , G=1)   S ( y_2| y_1 , w, x , g) ]\\
&= E [  \frac{ w g }{ p (G = 1) } ( y_2 - E [ Y_2 | y_1 , W = 1, X, G =1 ] )     \frac{ P( G =1 | Y_1 , W=1 , X ) }{ P(G=1 | Y_1 , W=1 , X ) } P( W=1 | X , G=1)   S ( y_2| y_1 , w, x , g) ]\\
&= E [  \frac{ w g }{ p (G = 1) } ( y_2 - E [ Y_2 | y_1 , W = 1, X, G =1 ] )     \frac{ P( G =1 | Y_1 , W=1 , X ) }{ P(G=1 | Y_1 , W=1 , X ) } P( W=1 | X , G=1)   S ( y_2| y_1 , w, x , g) ]\\
\end{align}
\begin{align}
\text{    Note that } \\
&=    E [  \frac{ w g }{ p (G = 1) } ( y_2 - E [ Y_2 | y_1 , W = 1, X, G =1 ] )   \\
&\frac{ P( G =1 | Y_1 , W=1 , X ) }{ P(G=1 | Y_1 , W=1 , X ) } P( W=1 | X , G=1)   S (  y_1 , w, x , g) ] = 0 \\
&\text{Therefore  }\\
&\sum_ { y_2 ,y_1,x} y_2 \partial_t p_t ( y_2| y_1 , W= 1, X , G= 1) p ( y_1 | W =1 , x, G= 0 ) p( x | G =1 ) \\
&= E [  \frac{ w g }{ p (G = 1) } ( y_2 - E [ Y_2 | y_1 , W = 1, X, G =1 ] )     \frac{ P( G =1 | Y_1 , W=1 , X ) }{ P(G=1 | Y_1 , W=1 , X ) } P( W=1 | X , G=1)   S ( V) ]
\end{align}


Likewise for the second term in \ref{eq: eif first chain}
\begin{align*}
    \sum_ { y_2 ,y_1,x} y_2  p ( y_2| y_1 , W= 1, X , G= 1)\partial_t p_t ( y_1 | W =1 , x, G= 0 ) p( x | G =1 ) \\
=   \sum_ { y_2 ,y_1,x} y_2  p ( y_2| y_1 , W= 1, X , G= 1) p ( y_1 | W =1 , x, G= 0 ) p( x | G =1 ) S( y_1  | W =1, x, G = 0)  \\
=   \sum_ { y_2 ,y_1,x} y_2  p ( y_2| y_1 , W= 1, X , G= 1) p ( y_1 | W =1 , x, G= 0 ) p( x | G =1 ) S( y_1  | W =1, x, G = 0)
\end{align*} 
noting that
\begin{align*}
p ( y_1(1) | w = 1, x g = 0 ) = \frac{ p ( y _1(1) , w = 1, g  = 0 |x) } { p ( G =0, W =1 | X) } = \frac{ w ( 1 -g) p( y_1(1) , w g | x) }{ p  (g =0, w =1 | x) } , \\
p ( x | G= 1) = \frac{ p  G =1, X ) } { p(G =1 ) },
\end{align*}
and rearranging terms,
\begin{align*}
&\sum_ { y_2 ,y_1,x} y_2  p ( y_2| y_1 , W= 1, X , G= 1)\partial_t p_t ( y_1 | W =1 , x, G= 0 ) p( x | G =1 ) \\ 
= &\sum_ { y_2 ,y_1,w,x, g} \frac{ w ( 1 -g ) y_2   p ( y_2| y_1 , W= 1, X , G= 1) p(G =1 | X) }{ p(G=1) p ( G= 0 | X)  p ( W = 1 | X ,G =0) } E [ Y_2 |   Y_1 ,W = 1,X, G =1 ]   S( y_1  | W , x, G )  \\
= &E \Square{   \frac{ W ( 1 -G)   p(G =1 | X) }{ p(G=1) p ( G= 0 | X)  p ( W = 1 | X ,G =0) } E [ Y_2 |   Y_1 ,W = 1, X, G =1 ]   S( y_1  | W , x, G )}\\
= &E \Square{  \frac{  W ( 1 -G)   p(G =1 | X) }{ p(G=1) p ( G= 0 | X)  p ( W = 1 | X ,G =0) } E [ Y_2 |   Y_1 ,W = 1, X, G =1 ]   S( y_1  | W , x, G ) }\\
= &E [  \frac{  W ( 1 -G)   p(G =1 | X) }{ p(G=1) p ( G= 0 | X)  p ( W = 1 | X ,G =0) } (E [ Y_2 |   Y_1 ,W = 1, X, G =1 ] - \\
&E [ E[ Y_2 | Y_1 , W =1 , X, G =1 ] | W=1, X, G= 0 ]] )  S( y_1  | W , x, G ) ]
\end{align*}
Note that,
\begin{align}
&[= E [  \frac{  W ( 1 -G)   p(G =1 | X) }{ p(G=1) p ( G= 0 | X)  p ( W = 1 | X ,G =0) } (E [ Y_2 |   Y_1 ,W = 1, X, G =1 ] - \\
&E [ E[ Y_2 | Y_1 , W =1 , X, G =1 ] | W=1, X, G= 0 ]] )  S(  W , x, G ) ] = 0]
\end{align}
therefore
\begin{align*}
&\sum_ { y_2 ,y_1,x} y_2  p ( y_2| y_1 , W= 1, X , G= 1)\partial_t p_t ( y_1 | W =1 , x, G= 0 ) p( x | G =1 ) \\ 
= &E [  \frac{  W ( 1 -G)   p(G =1 | X) }{ p(G=1) p ( G= 0 | X)  p ( W = 1 | X ,G =0) } (E [ Y_2 |   Y_1 ,W = 1, X, G =1 ] -\\
&E [ E[ Y_2 | Y_1 , W =1 , X, G =1 ] | W=1, X, G= 0 ]] )  S(  V) ] 
\end{align*}
for the third term
\begin{align*}
&\sum_ { y_2 ,y_1,x} y_2  p ( y_2| y_1 , W= 1, X , G= 1) p ( y_1 | W =1 , x, G= 0 ) \partial_t p_t( x | G =1 )\\
&= \sum_ { y_2 ,y_1,x} y_2  p ( y_2| y_1 , W= 1, X , G= 1) p ( y_1 | W =1 , x, G= 0 ) p( x | G =1 ) S( x | G= 1) \\
&= \sum_ { y_2 , y_1, x}      \frac{ G } { P(G =1 ) }   y_2  p ( y_2| y_1 , W= 1, X , G= 1)\\
&p ( y_1 | W =1 , x, G= 0 ) p( x | G =1 ) S( x | G= 1) \\
&= \sum_ { y_2 , y_1, x, g}      \frac{ G } { P(G =1 ) }   (E[ E[Y_2| Y_1,  W =1, X, G = 1 ] | W =1 , X, G = 0 ]]  - \\
&\tau_1 \todo) p( x , g ) S( x | G)
\end{align*}
note that
\begin{align*}
&= \sum_ { y_2 , y_1, x, g}      \frac{ G } { P(G =1 ) }   (E[ E[Y_2| Y_1,  W =1, X, G = 1 ] | W =1 , X, G = 0 ]]  - \\
&\tau_1 \todo) p( x , g ) S(  G) = 0
\end{align*}
therefore,
\begin{align*}
\sum_ { y_2 ,y_1,x} y_2  p ( y_2| y_1 , W= 1, X , G= 1) p ( y_1 | W =1 , x, G= 0 ) \partial_t p_t( x | G =1 )\\
= E[      \frac{ G } { P(G =1 ) }   (E[ E[Y_2| Y_1,  W =1, X, G = 1 ] | W =1 , X, G = 0 ]]  - \tau_1 \todo) p( x , g ) S(  V) ]
\end{align*}
collectivizing the three results, we have
\begin{align*}
&\partial_t \psi_t = E [  (\frac{ g w }{ p(G=1) } [ (y_2 - E[ Y_2 | W=1 , Y_1 , X, G =1]  ) \frac{ P( G =1 | Y_1 , W=1 , X ) }{ P(G=1 | Y_1 , W=1 , X ) } P( W=1 |X ,G=1) \\
& + E [ E[ Y_2 | W = 1, Y_1 , X, G =1 | W =1, X, G= 0 ] ] - \tau    ]  + \\
&\frac{ (1- g) P(G = 1|X)   }{ P(G= 1) P ( G =0 |X) } ( \frac{ w ( E [ Y _ 2 | Y_1,W =1 , X, G =1 ] - E [ E[ Y_2 | Y_1, W=1, X, G=1 ] | W =1, X, G= 0 ] ] ) }{ p ( W =1| X, G =0 ) } )S(V) ] 
\end{align*}
which implies that
\begin{align*}
&\frac{ g w }{ p(G=1) } [ (y_2 - E[ Y_2 | W=1 , Y_1 , X, G =1]  ) \frac{ P( G =1 | Y_1 , W=1 , X ) }{ P(G=1 | Y_1 , W=1 , X ) } P( W=1 | X , G=1) \\
&+ E [ E[ Y_2 | W= 1,Y_1 , X, G =1 | W =1, X, G= 0 ] ] - \tau    ]  + \\
&\frac{ (1- g) P(G = 1|X)   }{ P(G= 1) P ( G =0 |X) } ( \frac{ w ( E [ Y _ 2 | Y_1,W =1 , X, G =1 ] - E [ E[ Y_2 | Y_1, W=1, X, G=1 ] | W =1, X, G= 0 ] ] ) }{ p (W =1| X, G =0 ) }
\end{align*}

\end{proof}


- \subsection{ estimation of the duration models } 
    - in this section, we provide the semiparametric estimation of the duration parameters provided in  section \todo
    - However, it is well known that semiparametric duration models have several nonregular characteristics that make their estimation difficult. For example, \cite{hahn 1994} proved that the mixed proportional hazard model cannot be $ \sqrt {n} $ estimable under conditions by first showing how their model was part of the mixture model in \cite{chamberlain 1986} , and then indirelty using the result in\cite{ Pfanzagl  2000} . 
    - However, for our purposes, the alternative impossibility result based on classical semiparametric theory is sufficient. Specifically, using \cite{van der vaart 1991}, which shows that if a parameter is regular estimable,the norm of its pathwise derivative is bounded. Therefore, it is sufficient to show that the pathwise derivative of a parametric sub model has a unbounded norm in our problem which implies non $\sqrt{n} $ estimability. Below shows the formal proof \todo
    - 
    - In later work as in \cite{ridder and weid 2003} , they show that $\sqrt{n} $ estimability for certain classed semiparametric duration models is quite sensitive to the shape of the hazard around zero, and the limiting distribution varies greatly, affecting the finite sample behavior significantly. An effective method around still seems to be an open question, which may be part of the reason for the relative lack of empirical research using such methods. Therefore, I provide non asymptotic gaussian approximation method that is useable agnoistic to the limiting distribution, as an application of the seminal works of \cite{chernozhukov , 2012}
    - 
    - 
    - 


\bibliographystyle{apalike}
\bibliography{reference}
\end{document}